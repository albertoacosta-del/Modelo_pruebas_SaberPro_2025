{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8445708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# PIPELINE MODULAR FINAL PARA TRAIN Y TEST\n",
    "# ================================================\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "def preprocesar_pipeline(df, y_col=None, id_col_name=\"ID\",\n",
    "                         generar_csv_individual=True, train=True, prefijo_csv=\"train\"):\n",
    "    \"\"\"\n",
    "    Preprocesa un DataFrame, genera CSV individuales y completo.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de entrada.\n",
    "        y_col: nombre de la columna target (opcional para test).\n",
    "        id_col_name: nombre de la columna ID.\n",
    "        generar_csv_individual: si True, genera CSV por cada categórica.\n",
    "        train: si True, guarda objetos de fit para test.\n",
    "        prefijo_csv: prefijo de los archivos CSV generados.\n",
    "        \n",
    "    Returns:\n",
    "        df_completo: DataFrame preprocesado completo.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Separar ID y target\n",
    "    # ----------------------------\n",
    "    id_col = df[id_col_name].reset_index(drop=True)\n",
    "    \n",
    "    if y_col is not None:\n",
    "        y = df[y_col].reset_index(drop=True)\n",
    "        df = df.drop(columns=[y_col])\n",
    "    else:\n",
    "        y = None\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Eliminar columnas innecesarias\n",
    "    # ----------------------------\n",
    "    cols_a_eliminar = [\n",
    "        \"E_VALORMATRICULAUNIVERSIDAD\",\n",
    "        \"F_TIENELAVADORA\",\n",
    "        \"INDICADOR_1\",\n",
    "        \"INDICADOR_2\",\n",
    "        \"INDICADOR_3\",\n",
    "        \"INDICADOR_4\",\n",
    "        \"PERIODO_ACADEMICO\"\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in cols_a_eliminar if col in df.columns])\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Identificar tipos de columnas\n",
    "    # ----------------------------\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    if id_col_name in cat_cols:\n",
    "        cat_cols.remove(id_col_name)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Procesamiento numérico\n",
    "    # ----------------------------\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "    X_num = pd.DataFrame(imp_num.fit_transform(df[num_cols]), columns=num_cols)\n",
    "    scaler = RobustScaler()\n",
    "    X_num = pd.DataFrame(scaler.fit_transform(X_num), columns=num_cols)\n",
    "    X_num = X_num.reset_index(drop=True)\n",
    "    \n",
    "    # Guardar objetos si es train\n",
    "    if train:\n",
    "        joblib.dump(imp_num, \"imp_num.pkl\")\n",
    "        joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Procesamiento categórico\n",
    "    # ----------------------------\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    max_unique = 10\n",
    "    \n",
    "    all_cat_processed = []\n",
    "    ohe_dict = {}\n",
    "    freq_dict = {}\n",
    "    \n",
    "    y_reset = y.reset_index(drop=True) if y is not None else None\n",
    "    id_col_reset = id_col.reset_index(drop=True)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Imputar\n",
    "        col_data = imp_cat.fit_transform(df[[col]])\n",
    "        col_series = pd.Series(col_data.ravel(), name=col).reset_index(drop=True)\n",
    "        \n",
    "        # Codificación\n",
    "        if col_series.nunique() <= max_unique:\n",
    "            ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "            col_ohe = ohe.fit_transform(col_series.values.reshape(-1,1))\n",
    "            col_names = ohe.get_feature_names_out([col])\n",
    "            df_col = pd.DataFrame(col_ohe, columns=col_names).reset_index(drop=True)\n",
    "            ohe_dict[col] = ohe\n",
    "        else:\n",
    "            freq_map = col_series.value_counts(normalize=True)\n",
    "            df_col = col_series.map(freq_map).to_frame(name=f\"{col}_freq\").reset_index(drop=True)\n",
    "            freq_dict[col] = freq_map\n",
    "        \n",
    "        all_cat_processed.append(df_col)\n",
    "        \n",
    "        # CSV individual\n",
    "        if generar_csv_individual:\n",
    "            df_individual = pd.concat([X_num, df_col], axis=1)\n",
    "            df_individual[col + \"_original\"] = col_series\n",
    "            \n",
    "            # Evitar duplicado de ID\n",
    "            if \"ID\" in df_individual.columns:\n",
    "                df_individual.drop(columns=[\"ID\"], inplace=True)\n",
    "            \n",
    "            # Insertar ID y target correctamente alineados\n",
    "            df_individual.insert(0, \"ID\", id_col_reset)\n",
    "            if y is not None:\n",
    "                df_individual[\"RENDIMIENTO_GLOBAL\"] = y_reset\n",
    "            \n",
    "            df_individual.to_csv(f\"{prefijo_csv}preprocesado{col}.csv\", index=False)\n",
    "    \n",
    "    # Guardar objetos si es train\n",
    "    if train:\n",
    "        joblib.dump(imp_cat, \"imp_cat.pkl\")\n",
    "        joblib.dump(ohe_dict, \"ohe_dict.pkl\")\n",
    "        joblib.dump(freq_dict, \"freq_dict.pkl\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # CSV completo\n",
    "    # ----------------------------\n",
    "    all_cat_processed_reset = [df.reset_index(drop=True) for df in all_cat_processed]\n",
    "    df_completo = pd.concat([X_num] + all_cat_processed_reset, axis=1)\n",
    "    \n",
    "    # Evitar duplicado de ID\n",
    "    if \"ID\" in df_completo.columns:\n",
    "        df_completo.drop(columns=[\"ID\"], inplace=True)\n",
    "    \n",
    "    df_completo.insert(0, \"ID\", id_col_reset)\n",
    "    \n",
    "    if y is not None:\n",
    "        df_completo[\"RENDIMIENTO_GLOBAL\"] = y_reset\n",
    "    \n",
    "    df_completo.to_csv(f\"{prefijo_csv}_preprocesado_completo.csv\", index=False)\n",
    "    \n",
    "    return df_completo\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Ejemplo de uso para TRAIN\n",
    "# ------------------------------------------------\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train_proc = preprocesar_pipeline(df_train, y_col=\"RENDIMIENTO_GLOBAL\", id_col_name=\"ID\",\n",
    "                                     generar_csv_individual=True, train=True, prefijo_csv=\"train\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Ejemplo de uso para TEST (sin target)\n",
    "# ------------------------------------------------\n",
    "# df_test = pd.read_csv(\"test.csv\")\n",
    "# df_test_proc = preprocesar_pipeline(df_test, y_col=None, id_col_name=\"ID\",\n",
    "#                                     generar_csv_individual=False, train=False, prefijo_csv=\"test\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
